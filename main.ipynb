{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# TSV 파일 읽기\n",
    "with open(\"review.sorted.uniq.preprocessing.tok.bpe.tsv\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 데이터를 저장할 리스트 초기화\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# 각 줄을 처리\n",
    "for line in lines:\n",
    "    # 탭(\\t)으로 구분\n",
    "    split_line = line.strip().split('\\t')\n",
    "    \n",
    "    # 첫 번째 열: 레이블 처리\n",
    "    if split_line[0].lower() == 'positive':\n",
    "        labels.append(1)\n",
    "    elif split_line[0].lower() == 'negative':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {split_line[0]}\")\n",
    "    data.append(split_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 수: 150000\n",
      "단어수: 24630\n"
     ]
    }
   ],
   "source": [
    "## 리뷰 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer 객체 생성\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# sentences 데이터에 대한 피처 변환 수행\n",
    "# sentences는 분석할 텍스트 데이터의 리스트\n",
    "\n",
    "# neg는 앞에, pos는 뒤에 있기에 이를 합침\n",
    "# 30만개는 너무 커서 한번에 안담긴다...\n",
    "bag_size_t = 150000\n",
    "bag_size = int(bag_size_t/2)\n",
    "data_mini = data[:bag_size]+data[-bag_size:]\n",
    "labels_mini = labels[:bag_size] +labels[-bag_size:]\n",
    "features = count_vectorizer.fit_transform(data_mini)\n",
    "print(f\"document 수: {features.shape[0]}\")\n",
    "print(f\"단어수: {features.shape[1]}\")\n",
    "\n",
    "# features 객체를 NumPy 배열로 변환\n",
    "vectorized_sentences = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "# 벡터화된 문장과 피처 이름을 이용해 DataFrame 생성\n",
    "df = pd.DataFrame(vectorized_sentences, columns=feature_names)\n",
    "\n",
    "# 데이터프레임의 인덱스 이름 지정\n",
    "df.index.name = 'sentence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 문장으로 부터 상위 100 개 단어로 vocabulary 작성\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "# sentences에 포함된 문장들을 기반으로 단어의 토큰화를\n",
    "# 수행하며, 각 단어에 고유한 인덱스를 할당\n",
    "tokenizer.fit_on_texts(data_mini)\n",
    "\n",
    "# sentences 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(data_mini)\n",
    "\n",
    "# 시퀀스에 패딩 적용 (문장의 뒤쪽을 패딩하고, 필요시 뒤쪽을 잘라냄)\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reservoirpy as rpy\n",
    "from reservoirpy.nodes import Reservoir\n",
    "from reservoirpy.nodes import Reservoir, Ridge, Input\n",
    "import numpy as np\n",
    "\n",
    "ran = 50000\n",
    "# 단순한 +이어붙이기는 오류가 발생한다, padded는 2차원이기 때문이다\n",
    "X_train = np.concatenate([padded[:ran], padded[-ran:]], axis=0)\n",
    "Y_train = labels_mini[:ran] + labels_mini[-ran:]\n",
    "X_test = np.concatenate([padded[ran+1:ran+500], padded[-ran-500:-ran-1]], axis=0)\n",
    "Y_test = labels_mini[ran+1:ran+500] + labels_mini[-ran-500:-ran-1]\n",
    "\n",
    "source = Input()\n",
    "reservoir = Reservoir(500, sr=0.9, lr=0.1)\n",
    "readout = Ridge( ridge=1e-6)\n",
    "\n",
    "model = source >> reservoir >> readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-1: 100%|██████████| 1/1 [00:00<00:00, 674.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "states_train = []\n",
    "for x in X_train:\n",
    "    states = reservoir.run(x, reset=True)\n",
    "    states_train.append(states[-1, np.newaxis])\n",
    "    clear_output(wait=True)  # 출력 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ridge-1': Ridge(ridge=1e-06, input_bias=True, in=500, out=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readout.fit(states_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-1: 100%|██████████| 1/1 [00:00<00:00, 991.09it/s]\n",
      "Running Ridge-1: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = []\n",
    "for x in X_test:\n",
    "    states = reservoir.run(x, reset=True)\n",
    "    y = readout.run(states[-1, np.newaxis])\n",
    "    Y_pred.append(y)\n",
    "    clear_output(wait=True)  # 출력 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  69.038 %\n",
      "0.6903807615230461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "Y_pred_class = [1 if y_p[0] >= threshold else 0 for y_p in Y_pred]\n",
    "Y_test_class = [y_t for y_t in Y_test]\n",
    "\n",
    "score = accuracy_score(Y_test_class, Y_pred_class)\n",
    "\n",
    "print(\"Accuracy: \", f\"{score * 100:.3f} %\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.66268846]]), array([[0.64046051]]), array([[0.60275193]]), array([[0.62892907]]), array([[0.62892907]]), array([[0.66268846]]), array([[0.60275193]]), array([[0.66268846]]), array([[0.64046051]]), array([[0.64046051]])]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred[-10:])\n",
    "print(Y_test[-10:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
